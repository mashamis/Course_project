{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49839ea7",
   "metadata": {},
   "source": [
    "#### Тема: применение методов машинного обучения для предсказания биотоксичности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0d1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем все необходимые библиотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7d5e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#записываем smiles уже известных ингибиторов BCR-ABL тирозинкиназы и преобразовывем их в fingerprints\n",
    "nilotinib_smiles = \"CC1=C(C=C(C=C1)C(=O)NC2=CC(=CC(=C2)C(F)(F)F)N3C=C(N=C3)C)NC4=NC=CC(=N4)C5=CN=CC=C5\"\n",
    "nilotinib_mol = Chem.MolFromSmiles(nilotinib_smiles)\n",
    "nilotinib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)\n",
    "\n",
    "imatinib_smiles = \"CC1=C(C=C(C=C1)NC(=O)C2=CC=C(C=C2)CN3CCN(CC3)C)NC4=NC=CC(=N4)C5=CN=CC=C5\"\n",
    "imatinib_mol = Chem.MolFromSmiles(imatinib_smiles)\n",
    "imatinib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)\n",
    "\n",
    "dasatinib_smiles = \"CC1=C(C(=CC=C1)Cl)NC(=O)C2=CN=C(S2)NC3=CC(=NC(=N3)C)N4CCN(CC4)CCO\"\n",
    "dasatinib_mol = Chem.MolFromSmiles(dasatinib_smiles)\n",
    "dasatinib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)\n",
    "\n",
    "bosutinib_smiles = \"CN1CCN(CC1)CCCOC2=C(C=C3C(=C2)N=CC(=C3NC4=CC(=C(C=C4Cl)Cl)OC)C#N)OC\"\n",
    "bosutinib_mol = Chem.MolFromSmiles(bosutinib_smiles)\n",
    "bosutinib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)\n",
    "\n",
    "ponatinib_smiles = \"CC1=C(C=C(C=C1)C(=O)NC2=CC(=C(C=C2)CN3CCN(CC3)C)C(F)(F)F)C#CC4=CN=C5N4N=CC=C5\"\n",
    "ponatinib_mol = Chem.MolFromSmiles(ponatinib_smiles)\n",
    "ponatinib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)\n",
    "\n",
    "asciminib_smiles = \"C1CN(CC1O)C2=C(C=C(C=N2)C(=O)NC3=CC=C(C=C3)OC(F)(F)Cl)C4=CC=NN4\"\n",
    "asciminib_mol = Chem.MolFromSmiles(asciminib_smiles)\n",
    "asciminib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f9e0193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n",
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n",
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n",
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n",
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n",
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#создаём набор из ингибиторов для проверки качесства работы модели\n",
    "inhibitor_test = []\n",
    "fp_inhibitors = [nilotinib_fingerprint,\n",
    "                imatinib_fingerprint,\n",
    "                dasatinib_fingerprint,\n",
    "                bosutinib_fingerprint,\n",
    "                ponatinib_fingerprint,\n",
    "                asciminib_fingerprint]\n",
    "for fp in fp_inhibitors:\n",
    "    fp_str = fp.ToBitString()\n",
    "    print(fp_str + '\\n')\n",
    "    tmpX = np.array(list(fp_str),dtype=float)\n",
    "    inhibitor_test.append(tmpX)\n",
    "inhibitor_test = np.array(inhibitor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6ff87",
   "metadata": {},
   "source": [
    "Тут будет предсказываться гепатоксичность(токсичность для печени). В общем случае вещества по своей токсичности могут быть разделены на 6 классов:\n",
    "\n",
    "    - Класс 1 (A): смертельно при проглатывании (LD50 ≤ 5).\n",
    "    - Класс 2 (B): смертельно при проглатывании (5 < LD50 ≤ 50)\n",
    "    - Класс 3 (C): токсичен при проглатывании (50 < LD50 ≤ 300).\n",
    "    - Класс 4 (D): вреден при проглатывании (300 < LD50 ≤ 2000).\n",
    "    - Класс 5 (E): может быть вреден при проглатывании (2000 < LD50 ≤ 5000)\n",
    "    - Класс 6 (F): нетоксичный (LD50 > 5000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a28c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#уже известные классы биотоксичности представленных выше ингибиторов\n",
    "nilotinib_real_tox = 4\n",
    "imatinib_real_tox = 2\n",
    "dasatinib_real_tox = 4\n",
    "bosutinib_real_tox = 4\n",
    "ponatinib_real_tox = 5\n",
    "asciminib_real_tox = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd874e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#считываем данные и выделяем смайлз\n",
    "data = pd.read_csv('smiles_data.csv')\n",
    "smiles = data['smiles']\n",
    "\n",
    "#делим на тестовую и обучающую выборку\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "train_smiles = train_data['smiles']\n",
    "test_smiles = test_data['smiles']\n",
    "\n",
    "#создаём файл со всеми смайлз соединений обучающей выборки, чтобы определить их токсичность с помощью сервиса\n",
    "with open('train_smiles.csv', 'w') as file:\n",
    "    for item in train_smiles:\n",
    "        file.write(item + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "429eeea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('train_smiles.csv', 'w') as file:\n",
    "#    for item in train_smiles:\n",
    "#        file.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d305f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#преобразовывем все смайлз в fingerprints\n",
    "fingerprints_train = []\n",
    "for smile in train_smiles:\n",
    "    fingerprint = Chem.MolFromSmiles(smile)\n",
    "    fingerprints_train.append(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cef821",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints_test = []\n",
    "for smile in test_smiles:\n",
    "    fingerprint = Chem.MolFromSmiles(smile)\n",
    "    fingerprints_test.append(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb1f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавляем в тестовую и обучающую данные колонку с отпечатками\n",
    "train_data['fingerprint'] = fingerprints_train\n",
    "test_data['fingerprint'] = fingerprints_test\n",
    "\n",
    "#убираем значения None из списков\n",
    "fingerprints_train = [x for x in fingerprints_train if x is not None]\n",
    "fingerprints_test = [x for x in fingerprints_test if x is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b1ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#формируем train и test сеты так, чтобы можно было использовать модель\n",
    "X = []\n",
    "X_train = []\n",
    "for fp in fingerprints_train:\n",
    "    fingerprint = AllChem.GetMorganFingerprintAsBitVect(fp, radius=2, nBits=1024)\n",
    "    fp_str = fingerprint.ToBitString()\n",
    "    tmpX = np.array(list(fp_str),dtype=float)\n",
    "    X_train.append(tmpX)\n",
    "X = X_train\n",
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7d9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for fp in fingerprints_test:\n",
    "    fingerprint = AllChem.GetMorganFingerprintAsBitVect(fp, radius=2, nBits=1024)\n",
    "    fp_str = fingerprint.ToBitString()\n",
    "    tmpX = np.array(list(fp_str),dtype=float)\n",
    "    X_test.append(tmpX)\n",
    "    X.append(tmpX)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73f46b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6627beb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8c1f5",
   "metadata": {},
   "source": [
    "    В качестве тренировочной выборки имеем набор из 78823 канонических смайлз соединений (бинарными строками после обработки данных); \n",
    "\n",
    "    Для данных в используемом датасете не известны классы токсичночти, поэтому для дальнейшей работы с данными и проверки используемых моделей на тестовых данных можно использовать методы обучения без учителя или обучения с частичным контролем. В данном случае используем алгоритм кластеризации для группировки молекул на основе их структурных характеристик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1328b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cluster import KMeans\n",
    "#from rdkit import Chem\n",
    "#from rdkit.Chem import AllChem\n",
    "\n",
    "# Кластеризация\n",
    "#kmeans = KMeans(n_clusters=6)  # предполагаем 6 классов\n",
    "#kmeans.fit(X_train)\n",
    "#toxes_train = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d57c6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#toxes_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcf221c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "toxes_train = []\n",
    "targets = [1, 2, 3, 4, 5, 6]\n",
    "weights = [0.1, 0.2, 0.1, 0.5, 0.4, 0.2]\n",
    "amount = 78823\n",
    "for i in range(0, 78823):\n",
    "    tox = random.choices(targets, weights=weights, k=1)[0]\n",
    "    toxes_train.append(tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b36ff36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tox_classes.csv', 'w') as file:\n",
    "    for item in toxes_train:\n",
    "        file.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f09fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxes = pd.read_csv('tox_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a3607f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78823"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxes_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab3b3d",
   "metadata": {},
   "source": [
    "Machine learning models that are commonly used to predict biotoxicity of low-molecular compounds:\n",
    "\n",
    "    - Random forest classifier \n",
    "    В данной работе будет использована как уже готовая модель, так и своя её реализация\n",
    "    - K-nearest neighbors(kNN)\n",
    "    - Gradient Boosting: XGBoost, LightGBM, CatBoost\n",
    "    модели, основанные на градиентном бустинге, могут обеспечивать отличные результаты и обычно превосходят случайные леса на сложных задачах. Они помогают повысить производительность модели за счет последовательного обучения деревьев на остатковых ошибках.\n",
    "    \n",
    "    - Extra Trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0170fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#соединяем тестовую выборку и валидационные соединения для удобства\n",
    "X_both = np.concatenate((X_test, inhibitor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fde54b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#применяем классификатор Random forest\n",
    "classifier_1 = RandomForestClassifier(random_state=42)\n",
    "classifier_1.fit(X_train, toxes_train)\n",
    "tox_score_RF = classifier_1.predict(X_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e24c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!!!!!!!!применям написанный самостоятельно Random Forest (реализация будет написана ниже)\n",
    "#my_rf_classifier = MyRandomForestClassifier()\n",
    "#my_rf_classifier.fit(X_train, toxes_train)\n",
    "#tox_score_MyRF = my_rf_classifier.predict(X_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12fe3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#применяем классификатор KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, toxes_train)\n",
    "tox_score_KNN = knn.predict(X_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8acc4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#применяем XGBoost\n",
    "#import xgboost as xgb\n",
    "#xgb = xgb.XGBClassifier(use_label_encoded=False, eval_metric='logloss')\n",
    "#xgb.fit(X_train, toxes_train)\n",
    "#tox_score_XGB = xgb.predict(X_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24c5b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#применяем Extra Trees Classifier\n",
    "extra_trees = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "extra_trees.fit(X_train, toxes_train)\n",
    "tox_score_ET = extra_trees.predict(X_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d8b0f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#предсказываем класс для ингибиторов и вычисляем точность модели\n",
    "test_labels = [nilotinib_real_tox,\n",
    "               imatinib_real_tox,\n",
    "               dasatinib_real_tox,\n",
    "               bosutinib_real_tox,\n",
    "               ponatinib_real_tox ,\n",
    "               asciminib_real_tox]\n",
    "inhibitors_tox_pred_RF = tox_score_RF[-6:]\n",
    "inhibitors_tox_pred_KNN = tox_score_KNN[-6:]\n",
    "inhibitors_tox_pred_ET = tox_score_ET[-6:]\n",
    "#inhibitors_tox_pred_XGB = tox_score_XGB[-6:]\n",
    "#inhibitors_tox_pred_MyRF = tox_score_MyRF[-6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d54c3",
   "metadata": {},
   "source": [
    "Далее проведём оценку работы модели, сравнивая реальные значения классов токсичности для исследуемых ингибиторов:\n",
    "\n",
    "    - nilotinib\n",
    "    - imatinib\n",
    "    - dasatinib\n",
    "    - bosutinib\n",
    "    - ponatininb\n",
    "    - asciminib\n",
    " \n",
    "с полученными моделями значениями.\n",
    "\n",
    "Используем функцию `predict_proba` для получения вероятностных оценок принадлежности переданных объектов к каждому из классов. Это полезно, когда важно не только узнать, к какому классу относится объект, но и насколько уверенно модель делает это предсказание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d9cca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_rf = classifier_1.predict_proba(X_both)\n",
    "probs_knn = knn.predict_proba(X_both)\n",
    "probs_et = extra_trees.predict_proba(X_both)\n",
    "#probs_xgb = xgb.predict_proba(X_both)\n",
    "#probs_myrf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04b881cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs_et[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d06c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def count_entropy(y_real, y_proba):\n",
    "    if len(y_real) != len(y_proba):\n",
    "        raise ValueError(\"Длины y_real и y_proba должны совпадать.\")\n",
    "    \n",
    "    entropy = 0\n",
    "    for i in range(len(y_real)):\n",
    "        if y_proba[i] > 0:\n",
    "            entropy += -y_real[i] * math.log(y_proba[i])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97a5ee",
   "metadata": {},
   "source": [
    "Сюда надо пояснения добавить к тому, что тут вообще происходит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dac3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = [4, 2, 4, 4, 5, 5]\n",
    "y_real_encoded = [[0, 0, 0, 1, 0, 0],\n",
    "                  [0, 1, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 1, 0, 0],\n",
    "                  [0, 0, 0, 1, 0, 0],\n",
    "                  [0, 0, 0, 0, 1, 0],\n",
    "                  [0, 0, 0, 0, 1, 0]]\n",
    "y_probas_rf = probs_rf[-6:]\n",
    "y_probas_knn = probs_knn[-6:]\n",
    "y_probas_et = probs_et[-6:]\n",
    "#y_probas_myrf =\n",
    "#y_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94e099e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nilotinib: \n",
      "Random forest:  1.1086626245216111\n",
      "KNN:  1.6094379124341003\n",
      "Extra Trees classifier:  1.2378743560016174\n",
      "\n",
      "Imatinib: \n",
      "Random Forest 2.120263536200091\n",
      "KNN:  1.6094379124341003\n",
      "Extra Trees classifier:  1.6094379124341003\n",
      "\n",
      "Dasatinib: \n",
      "Random forest:  1.1086626245216111\n",
      "KNN:  1.6094379124341003\n",
      "Extra Trees classifier:  1.2378743560016174\n",
      "\n",
      "Bosutinib: \n",
      "Random forest:  1.1086626245216111\n",
      "KNN:  1.6094379124341003\n",
      "Extra Trees classifier:  1.2378743560016174\n",
      "\n",
      "Ponatininb: \n",
      "Random forest:  1.7147984280919266\n",
      "KNN:  1.6094379124341003\n",
      "Extra Trees classifier:  1.9661128563728327\n",
      "\n",
      "Asciminib: \n",
      "Random forest:  1.7147984280919266\n",
      "KNN:  1.6094379124341003\n",
      "Extra Trees classifier:  1.9661128563728327\n"
     ]
    }
   ],
   "source": [
    "#Вывод полученный значений энтропии\n",
    "print('Nilotinib: ')\n",
    "print('Random forest: ', count_entropy(y_real_encoded[0], y_probas_rf[0]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[0], y_probas_knn[0]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[0], y_probas_et[0]))\n",
    "#\n",
    "print()\n",
    "\n",
    "print('Imatinib: ')\n",
    "print('Random Forest', count_entropy(y_real_encoded[1], y_probas_rf[1]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[1], y_probas_knn[1]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[1], y_probas_et[1]))\n",
    "#\n",
    "print()\n",
    "\n",
    "print('Dasatinib: ')\n",
    "print('Random forest: ', count_entropy(y_real_encoded[2], y_probas_rf[2]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[2], y_probas_knn[2]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[2], y_probas_et[2]))\n",
    "#\n",
    "print()\n",
    "\n",
    "print('Bosutinib: ')\n",
    "print('Random forest: ', count_entropy(y_real_encoded[3], y_probas_rf[3]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[3], y_probas_knn[3]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[3], y_probas_et[3]))\n",
    "#\n",
    "print()\n",
    "\n",
    "print('Ponatininb: ')\n",
    "print('Random forest: ', count_entropy(y_real_encoded[4], y_probas_rf[4]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[4], y_probas_knn[4]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[4], y_probas_et[4]))\n",
    "#\n",
    "print()\n",
    "\n",
    "print('Asciminib: ')\n",
    "print('Random forest: ', count_entropy(y_real_encoded[5], y_probas_rf[5]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[5], y_probas_knn[5]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[4], y_probas_et[4]))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef2609",
   "metadata": {},
   "source": [
    "##### Оценка точности модели с помощью метрик\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a730ae5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest: \n",
      "Accuracy:  0.5\n",
      "Precision:  [0.  0.5 0. ]\n",
      "Recall RF:  [0. 1. 0.]\n",
      "\n",
      "KNN: \n",
      "Accuracy:  0.0\n",
      "Precision:  [0. 0. 0. 0.]\n",
      "Recall RF:  [0. 0. 0. 0.]\n",
      "\n",
      "KNN: \n",
      "Accuracy:  0.5\n",
      "Precision:  [0.  0.5 0. ]\n",
      "Recall RF:  [0. 1. 0.]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\misch\\anaconda3\\envs\\mipt-stats\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\misch\\anaconda3\\envs\\mipt-stats\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\misch\\anaconda3\\envs\\mipt-stats\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\misch\\anaconda3\\envs\\mipt-stats\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Random forest model\n",
    "accuracy_rf = accuracy_score(test_labels, inhibitors_tox_pred_RF)\n",
    "precision_rf = precision_score(test_labels, inhibitors_tox_pred_RF, average=None)\n",
    "recall_rf = recall_score(test_labels, inhibitors_tox_pred_RF, average=None)\n",
    "\n",
    "print(\"Random forest: \")\n",
    "print(\"Accuracy: \", accuracy_rf)\n",
    "print(\"Precision: \", precision_rf)\n",
    "print(\"Recall RF: \", recall_rf)\n",
    "print()\n",
    "\n",
    "#KNN model\n",
    "accuracy_knn = accuracy_score(test_labels, inhibitors_tox_pred_KNN)\n",
    "precision_knn = precision_score(test_labels, inhibitors_tox_pred_KNN, average=None)\n",
    "recall_knn = recall_score(test_labels, inhibitors_tox_pred_KNN, average=None)\n",
    "\n",
    "print(\"KNN: \")\n",
    "print(\"Accuracy: \", accuracy_knn)\n",
    "print(\"Precision: \", precision_knn)\n",
    "print(\"Recall RF: \", recall_knn)\n",
    "print()\n",
    "\n",
    "#Extra trees model\n",
    "accuracy_et = accuracy_score(test_labels, inhibitors_tox_pred_ET)\n",
    "precision_et = precision_score(test_labels, inhibitors_tox_pred_ET, average=None)\n",
    "recall_et = recall_score(test_labels, inhibitors_tox_pred_ET, average=None)\n",
    "\n",
    "print(\"KNN: \")\n",
    "print(\"Accuracy: \", accuracy_et)\n",
    "print(\"Precision: \", precision_et)\n",
    "print(\"Recall RF: \", recall_et)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d00d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c362a05",
   "metadata": {},
   "source": [
    "#### Реализация MyRandomForest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7f1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum \n",
    "import typing as tp \n",
    "import numpy as np \n",
    "from scipy.stats import mode \n",
    "\n",
    "class NodeType(enum.Enum):\n",
    "    REGULAR = 1\n",
    "    TERMINAL = 2\n",
    "\n",
    "\n",
    "def gini(y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes Gini index for given set of labels\n",
    "    :param y: labels\n",
    "    :return: Gini impurity\n",
    "    \"\"\"\n",
    "    total_size = len(y)\n",
    "    unique_labels, y_counts = np.unique(y, return_counts=True)\n",
    "    gini_index = 1.0;\n",
    "    for count in y_counts:\n",
    "        probability = count / total_size\n",
    "        gini_index -= pow(probability, 2)\n",
    "    return gini_index\n",
    "\n",
    "\n",
    "def weighted_impurity(y_left: np.ndarray, y_right: np.ndarray) -> \\\n",
    "        tp.Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Computes weighted impurity by averaging children impurities\n",
    "    :param y_left: left  partition\n",
    "    :param y_right: right partition\n",
    "    :return: averaged impurity, left child impurity, right child impurity\n",
    "    \"\"\"\n",
    "    total_size = len(y_left) + len(y_right)\n",
    "    \n",
    "    unique_labels_left, y_left_counts = np.unique(y_left, return_counts=True)\n",
    "    left_impurity = 1.0 - np.sum((y_left_counts / len(y_left)) ** 2)\n",
    "    \n",
    "    unique_labels_right, y_right_counts = np.unique(y_right, return_counts=True)\n",
    "    right_impurity = 1.0 - np.sum((y_right_counts / len(y_right)) ** 2)\n",
    "    \n",
    "    weighted_impurity = (left_impurity * len(y_left) + right_impurity * len(y_right))/total_size\n",
    "    \n",
    "    return weighted_impurity, left_impurity, right_impurity\n",
    "\n",
    "\n",
    "def create_split(feature_values: np.ndarray, threshold: float) -> tp.Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    splits given 1-d array according to relation to threshold into two subarrays\n",
    "    :param feature_values: feature values extracted from data\n",
    "    :param threshold: value to compare with\n",
    "    :return: two sets of indices\n",
    "    \"\"\"\n",
    "    left_idx = np.where(feature_values <= threshold)[0]\n",
    "    right_idx = np.where(feature_values > threshold)[0]\n",
    "    return left_idx, right_idx\n",
    "\n",
    "\n",
    "class MyDecisionTreeNode:\n",
    "    \"\"\"\n",
    "    Auxiliary class serving as representation of a decision tree node\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            meta: 'MyDecisionTreeClassifier',\n",
    "            depth,\n",
    "            node_type: NodeType = NodeType.REGULAR,\n",
    "            predicted_class: tp.Optional[tp.Union[int, str]] = None,\n",
    "            left_subtree: tp.Optional['MyDecisionTreeNode'] = None,\n",
    "            right_subtree: tp.Optional['MyDecisionTreeNode'] = None,\n",
    "            feature_id: int = None,\n",
    "            threshold: float = None,\n",
    "            impurity: float = np.inf\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param meta: object, holding meta information about tree\n",
    "        :param depth: depth of this node in a tree (is deduced on creation by depth of ancestor)\n",
    "        :param node_type: 'regular' or 'terminal' depending on whether this node is a leaf node\n",
    "        :param predicted_class: class label assigned to a terminal node\n",
    "        :param feature_id: index if feature to split by\n",
    "        :param\n",
    "        \"\"\"\n",
    "        self._node_type = node_type\n",
    "        self._meta = meta\n",
    "        self._depth = depth\n",
    "        self._predicted_class = predicted_class\n",
    "        self._class_proba = None\n",
    "        self._left_subtree = left_subtree\n",
    "        self._right_subtree = right_subtree\n",
    "        self._feature_id = feature_id\n",
    "        self._threshold = threshold\n",
    "        self._impurity = impurity\n",
    "\n",
    "    def _best_split(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        finds best split\n",
    "        :param X: Data, passed to node\n",
    "        :param y: labels\n",
    "        :return: best feature, best threshold, left child impurity, right child impurity\n",
    "        \"\"\"\n",
    "        lowest_impurity = np.inf\n",
    "        best_feature_id = None\n",
    "        best_threshold = None\n",
    "        lowest_left_child_impurity, lowest_right_child_impurity = None, None\n",
    "        features = self._meta.rng.permutation(X.shape[1])\n",
    "        for feature in features:\n",
    "            current_feature_values = X[:, feature]\n",
    "            thresholds = np.unique(current_feature_values)\n",
    "            for threshold in thresholds:\n",
    "                left_idx, right_idx = create_split(current_feature_values, threshold)\n",
    "                current_weighted_impurity, current_left_impurity, current_right_impurity = weighted_impurity(y[left_idx], y[right_idx])\n",
    "                if current_weighted_impurity < lowest_impurity:\n",
    "                    lowest_impurity = current_weighted_impurity\n",
    "                    best_feature_id = feature\n",
    "                    best_threshold = threshold\n",
    "                    lowest_left_child_impurity = current_left_impurity\n",
    "                    lowest_right_child_impurity = current_right_impurity\n",
    "\n",
    "        return best_feature_id, best_threshold, lowest_left_child_impurity, lowest_right_child_impurity\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        recursively fits a node, providing it with predicted class or split condition\n",
    "        :param X: Data\n",
    "        :param y: labels\n",
    "        :return: fitted node\n",
    "        \"\"\"\n",
    "        if len(np.unique(y)) == 1:\n",
    "            self._node_type = NodeType.TERMINAL\n",
    "            self._predicted_class = y[0]\n",
    "            self._class_proba = np.array([1.0])\n",
    "            return self\n",
    "\n",
    "        if self._depth >= self._meta.max_depth or len(X) < self._meta.min_samples_split:\n",
    "            self._node_type = NodeType.TERMINAL\n",
    "            self._predicted_class = mode(y).mode[0]\n",
    "            class_counts = np.bincount(y)\n",
    "            self._class_proba = class_counts / len(y)\n",
    "            return self\n",
    "\n",
    "        self._feature_id, self._threshold, left_imp, right_imp = self._best_split(X, y)\n",
    "        left_idx, right_idx = create_split(X[:, self._feature_id], self._threshold)\n",
    "        self._left_subtree = MyDecisionTreeNode(\n",
    "            meta=self._meta,\n",
    "            depth=self._depth + 1,\n",
    "            impurity=left_imp\n",
    "        ).fit(\n",
    "            X[left_idx],\n",
    "            y[left_idx]\n",
    "        )\n",
    "        self._right_subtree = MyDecisionTreeNode(\n",
    "            meta=self._meta,\n",
    "            depth=self._depth + 1,\n",
    "            impurity=right_imp\n",
    "        ).fit(\n",
    "            X[right_idx],\n",
    "            y[right_idx]\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Predicts class for a single object\n",
    "        :param x: object of shape (n_features, )\n",
    "        :return: class assigned to object\n",
    "        \"\"\"\n",
    "        if self._node_type is NodeType.TERMINAL:\n",
    "            return self._predicted_class\n",
    "        if x[self._feature_id] <= self._threshold:\n",
    "            return self._left_subtree.predict(x)\n",
    "        else:\n",
    "            return self._right_subtree.predict(x)\n",
    "\n",
    "    def predict_proba(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Predicts probability for a single object\n",
    "        :param x: object of shape (n_features, )\n",
    "        :return: vector of probabilities assigned to object\n",
    "        \"\"\"\n",
    "        if self._node_type is NodeType.TERMINAL:\n",
    "            return self._class_proba\n",
    "        if x[self._feature_id] <= self._threshold:\n",
    "            return self._left_subtree.predict_proba(x)\n",
    "        else:\n",
    "            return self._right_subtree.predict_proba(x)\n",
    "\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Class analogous to sklearn implementation of decision tree classifier with Gini impurity criterion,\n",
    "    named in a manner avoiding collisions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_depth: tp.Optional[int] = None,\n",
    "            min_samples_split: tp.Optional[int] = 2,\n",
    "            seed: int = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param max_depth: maximal depth of tree, prevents overfitting\n",
    "        :param min_samples_split: minimal amount of samples for node to be a splitter node\n",
    "        :param seed: seed for RNG, enables reproducibility\n",
    "        \"\"\"\n",
    "        self.root = MyDecisionTreeNode(self, 1)\n",
    "        self._is_trained = False\n",
    "        self.max_depth = max_depth or np.inf\n",
    "        self.min_samples_split = min_samples_split or 2\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self._n_classes = 0\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        starts recursive process of node criterion fitting from the root\n",
    "        :param X: Data\n",
    "        :param y: labels\n",
    "        :return: fitted self\n",
    "        \"\"\"\n",
    "        self._n_classes = np.unique(y).shape[0]\n",
    "        self.root.fit(X, y)\n",
    "        self._is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts class for a sequence of objects\n",
    "        :param x: Data\n",
    "        :return: classes assigned to each object\n",
    "        \"\"\"\n",
    "        if not self._is_trained:\n",
    "            raise RuntimeError('predict call on untrained model')\n",
    "        else:\n",
    "            predictions = np.array([self.root.predict(x) for x in X])\n",
    "            return predictions\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts class for a sequence of objects\n",
    "        :param x: Data\n",
    "        :return: probabilities of all classes for each object\n",
    "        \"\"\"\n",
    "        if not self._is_trained:\n",
    "            raise RuntimeError('predict call on untrained model')\n",
    "        else:\n",
    "            probas = np.array([self.root.predict_proba(x) for x in X])\n",
    "            return probas\n",
    "        \n",
    "class MyRandomForestClassifier:\n",
    "    \"\"\"\n",
    "    Data-diverse ensemble of tree calssifiers\n",
    "    \"\"\"\n",
    "    big_number = 1 << 32\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_estimators: int,\n",
    "            max_depth: tp.Optional[int] = None,\n",
    "            min_samples_split: tp.Optional[int] = 2,\n",
    "            seed: int = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param n_estimators: number of trees in forest\n",
    "        :param max_depth: maximal depth of tree, prevents overfitting\n",
    "        :param min_samples_split: minimal amount of samples for node to be a splitter node\n",
    "        :param seed: seed for RNG, enables reproducibility\n",
    "        \"\"\"\n",
    "        self._n_classes = 0\n",
    "        self._is_trained = False\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.estimators = [\n",
    "            MyDecisionTreeClassifier(max_depth, min_samples_split, seed=seed) for\n",
    "            seed in self.rng.choice(max(MyRandomForestClassifier.big_number, n_estimators), size=(n_estimators,),\n",
    "                                    replace=False)]\n",
    "\n",
    "    def _bootstrap_sample(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        returns bootstrapped sample from X of equal size\n",
    "        :param X: objects collection to sample from\n",
    "        :param y: corresponding labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        indices = self.rng.choice(X.shape[0], size=X.shape[0], replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        fits each estimator of the ensemble on the bootstrapped data sample\n",
    "        :param X: Data\n",
    "        :param y: labels\n",
    "        :return: fitted self\n",
    "        \"\"\"\n",
    "        self._n_classes = np.unique(y).shape[0]\n",
    "        for estimator in self.estimators:\n",
    "            X_boot, y_boot = self._bootstrap_sample(X, y)\n",
    "            estimator.fit(X_boot, y_boot)\n",
    "        self._is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        predict probability of each class by averaging over all base estimators\n",
    "        :param X: Data\n",
    "        :return: array of probabilities\n",
    "        \"\"\"\n",
    "        probas = np.zeros((X.shape[0], self._n_classes))\n",
    "        for estimator in self.estimators:\n",
    "            probas += estimator.predict_proba(X)\n",
    "        probas /= len(self.estimators)\n",
    "        return probas\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict class for each object\n",
    "        :param X: Data\n",
    "        :return: array of class labels\n",
    "        \"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return np.argmax(probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe02b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
