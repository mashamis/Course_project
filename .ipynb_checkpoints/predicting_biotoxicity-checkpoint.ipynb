{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49839ea7",
   "metadata": {},
   "source": [
    "#### Тема: применение методов машинного обучения для предсказания биотоксичности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9f0d1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем все необходимые библиотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7d5e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#записываем smiles уже известных ингибиторов BCR-ABL тирозинкиназы и преобразовывем их в fingerprints\n",
    "nilotinib_smiles = \"CC1=C(C=C(C=C1)C(=O)NC2=CC(=CC(=C2)C(F)(F)F)N3C=C(N=C3)C)NC4=NC=CC(=N4)C5=CN=CC=C5\"\n",
    "nilotinib_mol = Chem.MolFromSmiles(nilotinib_smiles)\n",
    "nilotinib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)\n",
    "\n",
    "imatinib_smiles = \"CC1=C(C=C(C=C1)NC(=O)C2=CC=C(C=C2)CN3CCN(CC3)C)NC4=NC=CC(=N4)C5=CN=CC=C5\"\n",
    "imatinib_mol = Chem.MolFromSmiles(imatinib_smiles)\n",
    "imatinib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)\n",
    "\n",
    "dasatinib_smiles = \"CC1=C(C(=CC=C1)Cl)NC(=O)C2=CN=C(S2)NC3=CC(=NC(=N3)C)N4CCN(CC4)CCO\"\n",
    "dasatinib_mol = Chem.MolFromSmiles(dasatinib_smiles)\n",
    "dasatinib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)\n",
    "\n",
    "bosutinib_smiles = \"CN1CCN(CC1)CCCOC2=C(C=C3C(=C2)N=CC(=C3NC4=CC(=C(C=C4Cl)Cl)OC)C#N)OC\"\n",
    "bosutinib_mol = Chem.MolFromSmiles(bosutinib_smiles)\n",
    "bosutinib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)\n",
    "\n",
    "ponatinib_smiles = \"CC1=C(C=C(C=C1)C(=O)NC2=CC(=C(C=C2)CN3CCN(CC3)C)C(F)(F)F)C#CC4=CN=C5N4N=CC=C5\"\n",
    "ponatinib_mol = Chem.MolFromSmiles(ponatinib_smiles)\n",
    "ponatinib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)\n",
    "\n",
    "asciminib_smiles = \"C1CN(CC1O)C2=C(C=C(C=N2)C(=O)NC3=CC=C(C=C3)OC(F)(F)Cl)C4=CC=NN4\"\n",
    "asciminib_mol = Chem.MolFromSmiles(asciminib_smiles)\n",
    "asciminib_fingerprint = AllChem.GetMorganFingerprintAsBitVect(nilotinib_mol, radius=2, nBits=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "4f9e0193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n",
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n",
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n",
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n",
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n",
      "0000000000000000000100000000000001000000000000000000000000000100100000000000000000001000001000000000000000000000001000000000000010000010100000000000000000000000000000000000000000000000000000010100000010000000000000000001000000000000000010000000000000000000010000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000100000100000000001000000000000001000001000000000000000000000000000011000000010000010000000000000010000010000000000000000000001000010000000000000000001100000000000000100000000000010000000000000001001000100000001100000001000000000000000100000000000000000000000000000000000000000000000000000010000000100000000100000100000000000000000000000010000000010000000000000000001000000000100000000000000000000000000000000001000000000000000100001001000001000001010000000000000000000001000000000010000001000000000000000000000000010000000000000000010000000000100000000000000000000000000010010000000000000000001000000000000000000000000010000000001000000000000000000000100000000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#создаём набор из ингибиторов для проверки качесства работы модели\n",
    "inhibitor_test = []\n",
    "fp_inhibitors = [nilotinib_fingerprint,\n",
    "                imatinib_fingerprint,\n",
    "                dasatinib_fingerprint,\n",
    "                bosutinib_fingerprint,\n",
    "                ponatinib_fingerprint,\n",
    "                asciminib_fingerprint]\n",
    "for fp in fp_inhibitors:\n",
    "    fp_str = fp.ToBitString()\n",
    "    print(fp_str + '\\n')\n",
    "    tmpX = np.array(list(fp_str),dtype=float)\n",
    "    inhibitor_test.append(tmpX)\n",
    "inhibitor_test = np.array(inhibitor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6ff87",
   "metadata": {},
   "source": [
    "Тут будет предсказываться гепатоксичность(токсичность для печени). В общем случае вещества по своей токсичности могут быть разделены на 6 классов:\n",
    "\n",
    "    - Класс 1 (A): смертельно при проглатывании (LD50 ≤ 5).\n",
    "    - Класс 2 (B): смертельно при проглатывании (5 < LD50 ≤ 50)\n",
    "    - Класс 3 (C): токсичен при проглатывании (50 < LD50 ≤ 300).\n",
    "    - Класс 4 (D): вреден при проглатывании (300 < LD50 ≤ 2000).\n",
    "    - Класс 5 (E): может быть вреден при проглатывании (2000 < LD50 ≤ 5000)\n",
    "    - Класс 6 (F): нетоксичный (LD50 > 5000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "6a28c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#уже известные классы биотоксичности представленных выше ингибиторов\n",
    "nilotinib_real_tox = 4\n",
    "imatinib_real_tox = 2\n",
    "dasatinib_real_tox = 4\n",
    "bosutinib_real_tox = 4\n",
    "ponatinib_real_tox = 5\n",
    "asciminib_real_tox = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "fd874e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#считываем данные и выделяем смайлз\n",
    "data = pd.read_csv('data/smiles_data.csv')\n",
    "smiles = data['smiles']\n",
    "\n",
    "#делим на тестовую и обучающую выборку\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "train_smiles = train_data['smiles']\n",
    "test_smiles = test_data['smiles']\n",
    "\n",
    "#создаём файл со всеми смайлз соединений обучающей выборки, чтобы определить их токсичность с помощью сервиса\n",
    "with open('train_smiles.csv', 'w') as file:\n",
    "    for item in train_smiles:\n",
    "        file.write(item + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "429eeea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('train_smiles.csv', 'w') as file:\n",
    "#    for item in train_smiles:\n",
    "#        file.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d305f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#преобразовывем все смайлз в fingerprints\n",
    "fingerprints_train = []\n",
    "for smile in train_smiles:\n",
    "    fingerprint = Chem.MolFromSmiles(smile)\n",
    "    fingerprints_train.append(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cef821",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints_test = []\n",
    "for smile in test_smiles:\n",
    "    fingerprint = Chem.MolFromSmiles(smile)\n",
    "    fingerprints_test.append(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb1f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавляем в тестовую и обучающую данные колонку с отпечатками\n",
    "train_data['fingerprint'] = fingerprints_train\n",
    "test_data['fingerprint'] = fingerprints_test\n",
    "\n",
    "#убираем значения None из списков\n",
    "fingerprints_train = [x for x in fingerprints_train if x is not None]\n",
    "fingerprints_test = [x for x in fingerprints_test if x is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "80b1ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#формируем train и test сеты так, чтобы можно было использовать модель\n",
    "X = []\n",
    "X_train = []\n",
    "for fp in fingerprints_train:\n",
    "    fingerprint = AllChem.GetMorganFingerprintAsBitVect(fp, radius=2, nBits=1024)\n",
    "    fp_str = fingerprint.ToBitString()\n",
    "    tmpX = np.array(list(fp_str),dtype=float)\n",
    "    X_train.append(tmpX)\n",
    "X = X_train\n",
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bc7d9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for fp in fingerprints_test:\n",
    "    fingerprint = AllChem.GetMorganFingerprintAsBitVect(fp, radius=2, nBits=1024)\n",
    "    fp_str = fingerprint.ToBitString()\n",
    "    tmpX = np.array(list(fp_str),dtype=float)\n",
    "    X_test.append(tmpX)\n",
    "    X.append(tmpX)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73f46b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627beb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c8c1f5",
   "metadata": {},
   "source": [
    "Готовый датасет с известными значениями для LD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6c890",
   "metadata": {},
   "source": [
    "    1) Используем датасет с уже известными значениями LD50 для каждого соединения\n",
    "    2) в датасете заданы Log(LD50) -> преобразуем их в LD50\n",
    "    2) исходя из LD50 определим класс токсичности для каждого из этих соединений\n",
    "    3) предсказываем классы токсичности для необходимых ингибиторов и тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb08638",
   "metadata": {},
   "source": [
    "##### Работаем с тренировочной выборкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f1328b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LD50_train = pd.read_csv('data/LD50_train.csv')\n",
    "LD50_train['label'] = 10**LD50_train['label']\n",
    "LD50_train['tox_classes'] = None\n",
    "\n",
    "for index, row in LD50_train.iterrows():\n",
    "    if row['label'] <= 5:\n",
    "        LD50_train.at[index, 'tox_classes'] = 1\n",
    "    elif row['label'] > 5 and row['label'] <= 50:\n",
    "        LD50_train.at[index, 'tox_classes'] = 2\n",
    "    elif row['label'] > 50 and row['label'] <= 300:\n",
    "        LD50_train.at[index, 'tox_classes'] = 3\n",
    "    elif row['label'] > 300 and row['label'] <= 2000:\n",
    "        LD50_train.at[index, 'tox_classes'] = 4\n",
    "    elif row['label'] > 2000 and row['label'] <= 5000:\n",
    "        LD50_train.at[index, 'tox_classes'] = 5\n",
    "    else:\n",
    "        LD50_train.at[index, 'tox_classes'] = 6\n",
    "        \n",
    "smiles_ld = LD50_train['smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "adb84194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         184.077200\n",
       "1         688.652296\n",
       "2          94.841846\n",
       "3          66.527316\n",
       "4          81.283052\n",
       "            ...     \n",
       "5926      156.675107\n",
       "5927      770.903469\n",
       "5928      654.636174\n",
       "5929    87700.082114\n",
       "5930      519.995997\n",
       "Name: label, Length: 5931, dtype: float64"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LD50_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "20dc4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразовывем все смайлз в fingerprints\n",
    "fp_train = []\n",
    "for smile in smiles_ld:\n",
    "    fingerprint = Chem.MolFromSmiles(smile)\n",
    "    fp_train.append(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7502fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "ld50_train = []\n",
    "for fp in fp_train:\n",
    "    fingerprint = AllChem.GetMorganFingerprintAsBitVect(fp, radius=2, nBits=1024)\n",
    "    fp_str = fingerprint.ToBitString()\n",
    "    tmpX = np.array(list(fp_str),dtype=float)\n",
    "    ld50_train.append(tmpX)\n",
    "X = ld50_train\n",
    "ld50_train = np.array(ld50_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d57c6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxes_train = LD50_train['tox_classes']\n",
    "toxes_train = toxes_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4d5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89e7b622",
   "metadata": {},
   "source": [
    "##### Преобразовывем тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "f50cef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "LD50_test = pd.read_csv('data/LD50_test.csv')\n",
    "LD50_test['label'] = 10**LD50_test['label']\n",
    "LD50_test['tox_classes'] = None\n",
    "\n",
    "for index, row in LD50_test.iterrows():\n",
    "    if row['label'] <= 5:\n",
    "        LD50_test.at[index, 'tox_classes'] = 1\n",
    "    elif row['label'] > 5 and row['label'] <= 50:\n",
    "        LD50_test.at[index, 'tox_classes'] = 2\n",
    "    elif row['label'] > 50 and row['label'] <= 300:\n",
    "        LD50_test.at[index, 'tox_classes'] = 3\n",
    "    elif row['label'] > 300 and row['label'] <= 2000:\n",
    "        LD50_test.at[index, 'tox_classes'] = 4\n",
    "    elif row['label'] > 2000 and row['label'] <= 5000:\n",
    "        LD50_test.at[index, 'tox_classes'] = 5\n",
    "    else:\n",
    "        LD50_test.at[index, 'tox_classes'] = 6\n",
    "\n",
    "smiles_ld_test = LD50_test['smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "fd8325a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9512092266663856\n"
     ]
    }
   ],
   "source": [
    "print(min(LD50_train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "7241f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразовывем все смайлз в fingerprints\n",
    "fp_test = []\n",
    "for smile in smiles_ld_test:\n",
    "    fingerprint = Chem.MolFromSmiles(smile)\n",
    "    fp_test.append(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7699a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "ld50_test = []\n",
    "for fp in fp_test:\n",
    "    fingerprint = AllChem.GetMorganFingerprintAsBitVect(fp, radius=2, nBits=1024)\n",
    "    fp_str = fingerprint.ToBitString()\n",
    "    tmpX = np.array(list(fp_str),dtype=float)\n",
    "    ld50_test.append(tmpX)\n",
    "X = ld50_test\n",
    "ld50_test = np.array(ld50_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "56abc339",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxes_test = LD50_test['tox_classes']\n",
    "toxes_test = toxes_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8d435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49827560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dcf221c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#toxes_train = []\n",
    "#targets = [1, 2, 3, 4, 5, 6]\n",
    "#weights = [0.1, 0.2, 0.1, 0.6, 0.4, 0.2]\n",
    "#amount = 78823\n",
    "#for i in range(0, 78824):\n",
    "#    tox = random.choices(targets, weights=weights, k=1)[0]\n",
    "#    toxes_train.append(tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cb26179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#toxes_test = []\n",
    "#weights = [0.1, 0.2, 0.1, 0.6, 0.4, 0.2]\n",
    "#amount = 33797\n",
    "#for i in range(0, 33798):\n",
    "#    tox = random.choices(targets, weights=weights, k=1)[0]\n",
    "#    toxes_test.append(tox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b36ff36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('tox_classes.csv', 'w') as file:\n",
    "#    for item in toxes_train:\n",
    "#        file.write(str(item) + '\\n')\n",
    "#        \n",
    "#with open('tox_classes_test.csv', 'w') as file:\n",
    "#    for item in toxes_test:\n",
    "#       file.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f09fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#toxes_train = pd.read_csv('tox_classes.csv')\n",
    "#toxes_test = pd.read_csv('tox_classes_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab3b3d",
   "metadata": {},
   "source": [
    "Machine learning methods that are commonly used to predict biotoxicity of low-molecular compounds:\n",
    "\n",
    "    - Random forest classifier \n",
    "    В данной работе будет использована как уже готовая модель, так и своя её реализация\n",
    "    \n",
    "    - K-nearest neighbors(kNN)\n",
    "    \n",
    "    - Extra Trees classifier\n",
    "    может работать быстрее по сравнению с RF, т.к. использует случайное разделение узлов (в то время как Random Forest затрачивает время на то, чтобы выбрать лучший узел для разделения)\n",
    "    \n",
    "    - Gradient Boosting: XGBoost, LightGBM, CatBoost\n",
    "    модели, основанные на градиентном бустинге, могут обеспечивать отличные результаты и обычно превосходят случайные леса на сложных задачах. Они помогают повысить производительность модели за счет последовательного обучения деревьев на остатковых ошибках.\n",
    "    \n",
    "    - Нейронные сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "0170fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#соединяем тестовую выборку и валидационные соединения для удобства\n",
    "#X_both = np.concatenate((X_test, inhibitor_test))\n",
    "X_both_ld = np.concatenate((ld50_test, inhibitor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fde54b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#применяем классификатор Random forest\n",
    "classifier_1 = RandomForestClassifier(random_state=42)\n",
    "classifier_1.fit(ld50_train, toxes_train)\n",
    "tox_score_RF = classifier_1.predict(X_both_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9e24c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#применям написанный самостоятельно Random Forest (реализация будет написана ниже)\n",
    "#my_rf_classifier = MyRandomForestClassifier(n_estimators = 200)\n",
    "#my_rf_classifier.fit(X_train, toxes_train)\n",
    "#tox_score_MyRF = my_rf_classifier.predict(X_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "12fe3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#применяем классификатор KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(ld50_train, toxes_train)\n",
    "tox_score_KNN = knn.predict(X_both_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "8acc4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#применяем XGBoost\n",
    "#import xgboost as xgb\n",
    "#xgb = xgb.XGBClassifier(use_label_encoded=False, eval_metric='logloss')\n",
    "#xgb.fit(X_train, toxes_train)\n",
    "#tox_score_XGB = xgb.predict(X_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "24c5b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#применяем Extra Trees Classifier\n",
    "extra_trees = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "extra_trees.fit(ld50_train, toxes_train)\n",
    "tox_score_ET = extra_trees.predict(X_both_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1d8b0f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#предсказываем класс для ингибиторов и вычисляем точность модели\n",
    "test_labels = [nilotinib_real_tox,\n",
    "               imatinib_real_tox,\n",
    "               dasatinib_real_tox,\n",
    "               bosutinib_real_tox,\n",
    "               ponatinib_real_tox ,\n",
    "               asciminib_real_tox]\n",
    "inhibitors_tox_pred_RF = tox_score_RF[-6:]\n",
    "inhibitors_tox_pred_KNN = tox_score_KNN[-6:]\n",
    "inhibitors_tox_pred_ET = tox_score_ET[-6:]\n",
    "#inhibitors_tox_pred_XGB = tox_score_XGB[-6:]\n",
    "#inhibitors_tox_pred_MyRF = tox_score_MyRF[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "1cb23d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_rf = tox_score_RF[:-6]\n",
    "y_test_pred_knn = tox_score_KNN[:-6]\n",
    "y_test_pred_et = tox_score_ET[:-6]\n",
    "\n",
    "y_test_real = toxes_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d54c3",
   "metadata": {},
   "source": [
    "Далее проведём оценку работы модели, сравнивая реальные значения классов токсичности для исследуемых ингибиторов:\n",
    "\n",
    "    - nilotinib\n",
    "    - imatinib\n",
    "    - dasatinib\n",
    "    - bosutinib\n",
    "    - ponatininb\n",
    "    - asciminib\n",
    " \n",
    "с полученными моделями значениями.\n",
    "\n",
    "Используем функцию `predict_proba` для получения вероятностных оценок принадлежности переданных объектов к каждому из классов. Это полезно, когда важно не только узнать, к какому классу относится объект, но и насколько уверенно модель делает это предсказание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "5d9cca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_rf = classifier_1.predict_proba(X_both)\n",
    "probs_knn = knn.predict_proba(X_both)\n",
    "probs_et = extra_trees.predict_proba(X_both)\n",
    "#probs_xgb = xgb.predict_proba(X_both)\n",
    "#probs_myrf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04b881cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs_et[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2d06c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def count_entropy(y_real, y_proba):\n",
    "    if len(y_real) != len(y_proba):\n",
    "        raise ValueError(\"Длины y_real и y_proba должны совпадать.\")\n",
    "    \n",
    "    entropy = 0\n",
    "    for i in range(len(y_real)):\n",
    "        if y_proba[i] > 0:\n",
    "            entropy += -y_real[i] * math.log(y_proba[i])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97a5ee",
   "metadata": {},
   "source": [
    "Сюда надо пояснения добавить к тому, что тут вообще происходит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "7dac3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = [4, 2, 4, 4, 5, 5]\n",
    "y_real_encoded = [[0, 0, 0, 1, 0, 0],\n",
    "                  [0, 1, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 1, 0, 0],\n",
    "                  [0, 0, 0, 1, 0, 0],\n",
    "                  [0, 0, 0, 0, 1, 0],\n",
    "                  [0, 0, 0, 0, 1, 0]]\n",
    "y_probas_rf = probs_rf[-6:]\n",
    "y_probas_knn = probs_knn[-6:]\n",
    "y_probas_et = probs_et[-6:]\n",
    "#y_probas_myrf =\n",
    "#y_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ae812756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.03, 0.36, 0.39, 0.1 , 0.12],\n",
       "       [0.  , 0.03, 0.36, 0.39, 0.1 , 0.12],\n",
       "       [0.  , 0.03, 0.36, 0.39, 0.1 , 0.12],\n",
       "       [0.  , 0.03, 0.36, 0.39, 0.1 , 0.12],\n",
       "       [0.  , 0.03, 0.36, 0.39, 0.1 , 0.12],\n",
       "       [0.  , 0.03, 0.36, 0.39, 0.1 , 0.12]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probas_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b18a5a",
   "metadata": {},
   "source": [
    "#### Вывод Multiclass-Cross Entropy для каждого соединения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "94e099e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nilotinib: \n",
      "Random forest:  0.941608539858445\n",
      "KNN:  1.6094379124341003\n",
      "Extra Trees classifier:  1.2494351784026934\n",
      "\n",
      "Imatinib: \n",
      "Random Forest 3.506557897319982\n",
      "KNN:  0.0\n",
      "Extra Trees classifier:  2.4079456086518722\n",
      "\n",
      "Dasatinib: \n",
      "Random forest:  0.941608539858445\n",
      "KNN:  1.6094379124341003\n",
      "Extra Trees classifier:  1.2494351784026934\n",
      "\n",
      "Bosutinib: \n",
      "Random forest:  0.941608539858445\n",
      "KNN:  1.6094379124341003\n",
      "Extra Trees classifier:  1.2494351784026934\n",
      "\n",
      "Ponatininb: \n",
      "Random forest:  2.3025850929940455\n",
      "KNN:  0.0\n",
      "Extra Trees classifier:  1.8119621765455742\n",
      "\n",
      "Asciminib: \n",
      "Random forest:  2.3025850929940455\n",
      "KNN:  0.0\n",
      "Extra Trees classifier:  1.8119621765455742\n"
     ]
    }
   ],
   "source": [
    "print('Nilotinib: ')\n",
    "print('Random forest: ', count_entropy(y_real_encoded[0], y_probas_rf[0]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[0], y_probas_knn[0]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[0], y_probas_et[0]))\n",
    "#\n",
    "print()\n",
    "\n",
    "print('Imatinib: ')\n",
    "print('Random Forest', count_entropy(y_real_encoded[1], y_probas_rf[1]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[1], y_probas_knn[1]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[1], y_probas_et[1]))\n",
    "#\n",
    "print()\n",
    "\n",
    "print('Dasatinib: ')\n",
    "print('Random forest: ', count_entropy(y_real_encoded[2], y_probas_rf[2]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[2], y_probas_knn[2]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[2], y_probas_et[2]))\n",
    "#\n",
    "print()\n",
    "\n",
    "print('Bosutinib: ')\n",
    "print('Random forest: ', count_entropy(y_real_encoded[3], y_probas_rf[3]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[3], y_probas_knn[3]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[3], y_probas_et[3]))\n",
    "#\n",
    "print()\n",
    "\n",
    "print('Ponatininb: ')\n",
    "print('Random forest: ', count_entropy(y_real_encoded[4], y_probas_rf[4]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[4], y_probas_knn[4]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[4], y_probas_et[4]))\n",
    "#\n",
    "print()\n",
    "\n",
    "print('Asciminib: ')\n",
    "print('Random forest: ', count_entropy(y_real_encoded[5], y_probas_rf[5]))\n",
    "print('KNN: ', count_entropy(y_real_encoded[5], y_probas_knn[5]))\n",
    "print('Extra Trees classifier: ', count_entropy(y_real_encoded[4], y_probas_et[4]))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0dc90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bef2609",
   "metadata": {},
   "source": [
    "##### Оценка точности предсказаний модели с помощью метрик для всех тестовых данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "29f95387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest: \n",
      "Accuracy:  0.5425101214574899\n",
      "Precision:  [0.         0.57058824 0.52234994 0.51290323 0.38235294 0.68108108]\n",
      "Recall RF:  [0.         0.39271255 0.73693694 0.43089431 0.10833333 0.68108108]\n",
      "\n",
      "KNN: \n",
      "Accuracy:  0.47435897435897434\n",
      "Precision:  [0.         0.36315789 0.51482059 0.4352518  0.39393939 0.68707483]\n",
      "Recall KNN:  [0.         0.55870445 0.59459459 0.32791328 0.10833333 0.54594595]\n",
      "\n",
      "Extra Trees: \n",
      "Accuracy:  0.5472334682860999\n",
      "Precision:  [0.         0.55675676 0.5397039  0.50613497 0.39583333 0.68333333]\n",
      "Recall ET:  [0.         0.41700405 0.72252252 0.44715447 0.15833333 0.66486486]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\misch\\anaconda3\\envs\\mipt-stats\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\misch\\anaconda3\\envs\\mipt-stats\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Random forest model\n",
    "accuracy_rf = accuracy_score(y_test_real, y_test_pred_rf)\n",
    "precision_rf = precision_score(y_test_real, y_test_pred_rf, average=None)\n",
    "recall_rf = recall_score(y_test_real, y_test_pred_rf, average=None)\n",
    "\n",
    "print(\"Random forest: \")\n",
    "print(\"Accuracy: \", accuracy_rf)\n",
    "print(\"Precision: \", precision_rf)\n",
    "print(\"Recall RF: \", recall_rf)\n",
    "print()\n",
    "\n",
    "#KNN model\n",
    "accuracy_knn = accuracy_score(y_test_real, y_test_pred_knn)\n",
    "precision_knn = precision_score(y_test_real, y_test_pred_knn, average=None)\n",
    "recall_knn = recall_score(y_test_real, y_test_pred_knn, average=None)\n",
    "\n",
    "print(\"KNN: \")\n",
    "print(\"Accuracy: \", accuracy_knn)\n",
    "print(\"Precision: \", precision_knn)\n",
    "print(\"Recall KNN: \", recall_knn)\n",
    "print()\n",
    "\n",
    "#Extra trees model\n",
    "accuracy_et = accuracy_score(y_test_real, y_test_pred_et)\n",
    "precision_et = precision_score(y_test_real, y_test_pred_et, average=None)\n",
    "recall_et = recall_score(y_test_real, y_test_pred_et, average=None)\n",
    "\n",
    "print(\"Extra Trees: \")\n",
    "print(\"Accuracy: \", accuracy_et)\n",
    "print(\"Precision: \", precision_et)\n",
    "print(\"Recall ET: \", recall_et)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea5e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f0f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6575fd6",
   "metadata": {},
   "source": [
    "##### Оценка точности предсказаний моделей  для 6 необходимых ингибиторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a730ae5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\naccuracy_rf = accuracy_score(test_labels, inhibitors_tox_pred_RF)\\nprecision_rf = precision_score(test_labels, inhibitors_tox_pred_RF, average=None)\\nrecall_rf = recall_score(test_labels, inhibitors_tox_pred_RF, average=None)\\n\\nprint(\"Random forest: \")\\nprint(\"Accuracy: \", accuracy_rf)\\nprint(\"Precision: \", precision_rf)\\nprint(\"Recall RF: \", recall_rf)\\nprint()\\n\\n#KNN model\\naccuracy_knn = accuracy_score(test_labels, inhibitors_tox_pred_KNN)\\nprecision_knn = precision_score(test_labels, inhibitors_tox_pred_KNN, average=None)\\nrecall_knn = recall_score(test_labels, inhibitors_tox_pred_KNN, average=None)\\n\\nprint(\"KNN: \")\\nprint(\"Accuracy: \", accuracy_knn)\\nprint(\"Precision: \", precision_knn)\\nprint(\"Recall RF: \", recall_knn)\\nprint()\\n\\n#Extra trees model\\naccuracy_et = accuracy_score(test_labels, inhibitors_tox_pred_ET)\\nprecision_et = precision_score(test_labels, inhibitors_tox_pred_ET, average=None)\\nrecall_et = recall_score(test_labels, inhibitors_tox_pred_ET, average=None)\\n\\nprint(\"ET: \")\\nprint(\"Accuracy: \", accuracy_et)\\nprint(\"Precision: \", precision_et)\\nprint(\"Recall RF: \", recall_et)\\nprint()\\n'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model\n",
    "'''\n",
    "accuracy_rf = accuracy_score(test_labels, inhibitors_tox_pred_RF)\n",
    "precision_rf = precision_score(test_labels, inhibitors_tox_pred_RF, average=None)\n",
    "recall_rf = recall_score(test_labels, inhibitors_tox_pred_RF, average=None)\n",
    "\n",
    "print(\"Random forest: \")\n",
    "print(\"Accuracy: \", accuracy_rf)\n",
    "print(\"Precision: \", precision_rf)\n",
    "print(\"Recall RF: \", recall_rf)\n",
    "print()\n",
    "\n",
    "#KNN model\n",
    "accuracy_knn = accuracy_score(test_labels, inhibitors_tox_pred_KNN)\n",
    "precision_knn = precision_score(test_labels, inhibitors_tox_pred_KNN, average=None)\n",
    "recall_knn = recall_score(test_labels, inhibitors_tox_pred_KNN, average=None)\n",
    "\n",
    "print(\"KNN: \")\n",
    "print(\"Accuracy: \", accuracy_knn)\n",
    "print(\"Precision: \", precision_knn)\n",
    "print(\"Recall RF: \", recall_knn)\n",
    "print()\n",
    "\n",
    "#Extra trees model\n",
    "accuracy_et = accuracy_score(test_labels, inhibitors_tox_pred_ET)\n",
    "precision_et = precision_score(test_labels, inhibitors_tox_pred_ET, average=None)\n",
    "recall_et = recall_score(test_labels, inhibitors_tox_pred_ET, average=None)\n",
    "\n",
    "print(\"ET: \")\n",
    "print(\"Accuracy: \", accuracy_et)\n",
    "print(\"Precision: \", precision_et)\n",
    "print(\"Recall RF: \", recall_et)\n",
    "print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d00d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c362a05",
   "metadata": {},
   "source": [
    "#### Реализация MyRandomForest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e7f1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum \n",
    "import typing as tp \n",
    "import numpy as np \n",
    "from scipy.stats import mode \n",
    "\n",
    "class NodeType(enum.Enum):\n",
    "    REGULAR = 1\n",
    "    TERMINAL = 2\n",
    "\n",
    "\n",
    "def gini(y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes Gini index for given set of labels\n",
    "    :param y: labels\n",
    "    :return: Gini impurity\n",
    "    \"\"\"\n",
    "    total_size = len(y)\n",
    "    unique_labels, y_counts = np.unique(y, return_counts=True)\n",
    "    gini_index = 1.0;\n",
    "    for count in y_counts:\n",
    "        probability = count / total_size\n",
    "        gini_index -= pow(probability, 2)\n",
    "    return gini_index\n",
    "\n",
    "\n",
    "def weighted_impurity(y_left: np.ndarray, y_right: np.ndarray) -> \\\n",
    "        tp.Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Computes weighted impurity by averaging children impurities\n",
    "    :param y_left: left  partition\n",
    "    :param y_right: right partition\n",
    "    :return: averaged impurity, left child impurity, right child impurity\n",
    "    \"\"\"\n",
    "    total_size = len(y_left) + len(y_right)\n",
    "    \n",
    "    unique_labels_left, y_left_counts = np.unique(y_left, return_counts=True)\n",
    "    left_impurity = 1.0 - np.sum((y_left_counts / len(y_left)) ** 2)\n",
    "    \n",
    "    unique_labels_right, y_right_counts = np.unique(y_right, return_counts=True)\n",
    "    right_impurity = 1.0 - np.sum((y_right_counts / len(y_right)) ** 2)\n",
    "    \n",
    "    weighted_impurity = (left_impurity * len(y_left) + right_impurity * len(y_right))/total_size\n",
    "    \n",
    "    return weighted_impurity, left_impurity, right_impurity\n",
    "\n",
    "\n",
    "def create_split(feature_values: np.ndarray, threshold: float) -> tp.Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    splits given 1-d array according to relation to threshold into two subarrays\n",
    "    :param feature_values: feature values extracted from data\n",
    "    :param threshold: value to compare with\n",
    "    :return: two sets of indices\n",
    "    \"\"\"\n",
    "    left_idx = np.where(feature_values <= threshold)[0]\n",
    "    right_idx = np.where(feature_values > threshold)[0]\n",
    "    return left_idx, right_idx\n",
    "\n",
    "\n",
    "class MyDecisionTreeNode:\n",
    "    \"\"\"\n",
    "    Auxiliary class serving as representation of a decision tree node\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            meta: 'MyDecisionTreeClassifier',\n",
    "            depth,\n",
    "            node_type: NodeType = NodeType.REGULAR,\n",
    "            predicted_class: tp.Optional[tp.Union[int, str]] = None,\n",
    "            left_subtree: tp.Optional['MyDecisionTreeNode'] = None,\n",
    "            right_subtree: tp.Optional['MyDecisionTreeNode'] = None,\n",
    "            feature_id: int = None,\n",
    "            threshold: float = None,\n",
    "            impurity: float = np.inf\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param meta: object, holding meta information about tree\n",
    "        :param depth: depth of this node in a tree (is deduced on creation by depth of ancestor)\n",
    "        :param node_type: 'regular' or 'terminal' depending on whether this node is a leaf node\n",
    "        :param predicted_class: class label assigned to a terminal node\n",
    "        :param feature_id: index if feature to split by\n",
    "        :param\n",
    "        \"\"\"\n",
    "        self._node_type = node_type\n",
    "        self._meta = meta\n",
    "        self._depth = depth\n",
    "        self._predicted_class = predicted_class\n",
    "        self._class_proba = None\n",
    "        self._left_subtree = left_subtree\n",
    "        self._right_subtree = right_subtree\n",
    "        self._feature_id = feature_id\n",
    "        self._threshold = threshold\n",
    "        self._impurity = impurity\n",
    "\n",
    "    def _best_split(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        finds best split\n",
    "        :param X: Data, passed to node\n",
    "        :param y: labels\n",
    "        :return: best feature, best threshold, left child impurity, right child impurity\n",
    "        \"\"\"\n",
    "        lowest_impurity = np.inf\n",
    "        best_feature_id = None\n",
    "        best_threshold = None\n",
    "        lowest_left_child_impurity, lowest_right_child_impurity = None, None\n",
    "        features = self._meta.rng.permutation(X.shape[1])\n",
    "        for feature in features:\n",
    "            current_feature_values = X[:, feature]\n",
    "            thresholds = np.unique(current_feature_values)\n",
    "            for threshold in thresholds:\n",
    "                left_idx, right_idx = create_split(current_feature_values, threshold)\n",
    "                current_weighted_impurity, current_left_impurity, current_right_impurity = weighted_impurity(y[left_idx], y[right_idx])\n",
    "                if current_weighted_impurity < lowest_impurity:\n",
    "                    lowest_impurity = current_weighted_impurity\n",
    "                    best_feature_id = feature\n",
    "                    best_threshold = threshold\n",
    "                    lowest_left_child_impurity = current_left_impurity\n",
    "                    lowest_right_child_impurity = current_right_impurity\n",
    "\n",
    "        return best_feature_id, best_threshold, lowest_left_child_impurity, lowest_right_child_impurity\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        recursively fits a node, providing it with predicted class or split condition\n",
    "        :param X: Data\n",
    "        :param y: labels\n",
    "        :return: fitted node\n",
    "        \"\"\"\n",
    "        if len(np.unique(y)) == 1:\n",
    "            self._node_type = NodeType.TERMINAL\n",
    "            self._predicted_class = y[0]\n",
    "            self._class_proba = np.array([1.0])\n",
    "            return self\n",
    "\n",
    "        if self._depth >= self._meta.max_depth or len(X) < self._meta.min_samples_split:\n",
    "            self._node_type = NodeType.TERMINAL\n",
    "            self._predicted_class = mode(y).mode[0]\n",
    "            class_counts = np.bincount(y)\n",
    "            self._class_proba = class_counts / len(y)\n",
    "            return self\n",
    "\n",
    "        self._feature_id, self._threshold, left_imp, right_imp = self._best_split(X, y)\n",
    "        left_idx, right_idx = create_split(X[:, self._feature_id], self._threshold)\n",
    "        self._left_subtree = MyDecisionTreeNode(\n",
    "            meta=self._meta,\n",
    "            depth=self._depth + 1,\n",
    "            impurity=left_imp\n",
    "        ).fit(\n",
    "            X[left_idx],\n",
    "            y[left_idx]\n",
    "        )\n",
    "        self._right_subtree = MyDecisionTreeNode(\n",
    "            meta=self._meta,\n",
    "            depth=self._depth + 1,\n",
    "            impurity=right_imp\n",
    "        ).fit(\n",
    "            X[right_idx],\n",
    "            y[right_idx]\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Predicts class for a single object\n",
    "        :param x: object of shape (n_features, )\n",
    "        :return: class assigned to object\n",
    "        \"\"\"\n",
    "        if self._node_type is NodeType.TERMINAL:\n",
    "            return self._predicted_class\n",
    "        if x[self._feature_id] <= self._threshold:\n",
    "            return self._left_subtree.predict(x)\n",
    "        else:\n",
    "            return self._right_subtree.predict(x)\n",
    "\n",
    "    def predict_proba(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Predicts probability for a single object\n",
    "        :param x: object of shape (n_features, )\n",
    "        :return: vector of probabilities assigned to object\n",
    "        \"\"\"\n",
    "        if self._node_type is NodeType.TERMINAL:\n",
    "            return self._class_proba\n",
    "        if x[self._feature_id] <= self._threshold:\n",
    "            return self._left_subtree.predict_proba(x)\n",
    "        else:\n",
    "            return self._right_subtree.predict_proba(x)\n",
    "\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Class analogous to sklearn implementation of decision tree classifier with Gini impurity criterion,\n",
    "    named in a manner avoiding collisions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_depth: tp.Optional[int] = None,\n",
    "            min_samples_split: tp.Optional[int] = 2,\n",
    "            seed: int = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param max_depth: maximal depth of tree, prevents overfitting\n",
    "        :param min_samples_split: minimal amount of samples for node to be a splitter node\n",
    "        :param seed: seed for RNG, enables reproducibility\n",
    "        \"\"\"\n",
    "        self.root = MyDecisionTreeNode(self, 1)\n",
    "        self._is_trained = False\n",
    "        self.max_depth = max_depth or np.inf\n",
    "        self.min_samples_split = min_samples_split or 2\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self._n_classes = 0\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        starts recursive process of node criterion fitting from the root\n",
    "        :param X: Data\n",
    "        :param y: labels\n",
    "        :return: fitted self\n",
    "        \"\"\"\n",
    "        self._n_classes = np.unique(y).shape[0]\n",
    "        self.root.fit(X, y)\n",
    "        self._is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts class for a sequence of objects\n",
    "        :param x: Data\n",
    "        :return: classes assigned to each object\n",
    "        \"\"\"\n",
    "        if not self._is_trained:\n",
    "            raise RuntimeError('predict call on untrained model')\n",
    "        else:\n",
    "            predictions = np.array([self.root.predict(x) for x in X])\n",
    "            return predictions\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts class for a sequence of objects\n",
    "        :param x: Data\n",
    "        :return: probabilities of all classes for each object\n",
    "        \"\"\"\n",
    "        if not self._is_trained:\n",
    "            raise RuntimeError('predict call on untrained model')\n",
    "        else:\n",
    "            probas = np.array([self.root.predict_proba(x) for x in X])\n",
    "            return probas\n",
    "        \n",
    "class MyRandomForestClassifier:\n",
    "    \"\"\"\n",
    "    Data-diverse ensemble of tree calssifiers\n",
    "    \"\"\"\n",
    "    big_number = 1 << 32\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_estimators: int,\n",
    "            max_depth: tp.Optional[int] = None,\n",
    "            min_samples_split: tp.Optional[int] = 2,\n",
    "            seed: int = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param n_estimators: number of trees in forest\n",
    "        :param max_depth: maximal depth of tree, prevents overfitting\n",
    "        :param min_samples_split: minimal amount of samples for node to be a splitter node\n",
    "        :param seed: seed for RNG, enables reproducibility\n",
    "        \"\"\"\n",
    "        self._n_classes = 0\n",
    "        self._is_trained = False\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.estimators = [\n",
    "            MyDecisionTreeClassifier(max_depth, min_samples_split, seed=seed) for\n",
    "            seed in self.rng.choice(max(MyRandomForestClassifier.big_number, n_estimators), size=(n_estimators,),\n",
    "                                    replace=False)]\n",
    "\n",
    "    def _bootstrap_sample(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        returns bootstrapped sample from X of equal size\n",
    "        :param X: objects collection to sample from\n",
    "        :param y: corresponding labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        indices = self.rng.choice(X.shape[0], size=X.shape[0], replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        fits each estimator of the ensemble on the bootstrapped data sample\n",
    "        :param X: Data\n",
    "        :param y: labels\n",
    "        :return: fitted self\n",
    "        \"\"\"\n",
    "        self._n_classes = np.unique(y).shape[0]\n",
    "        for estimator in self.estimators:\n",
    "            X_boot, y_boot = self._bootstrap_sample(X, y)\n",
    "            estimator.fit(X_boot, y_boot)\n",
    "        self._is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        predict probability of each class by averaging over all base estimators\n",
    "        :param X: Data\n",
    "        :return: array of probabilities\n",
    "        \"\"\"\n",
    "        probas = np.zeros((X.shape[0], self._n_classes))\n",
    "        for estimator in self.estimators:\n",
    "            probas += estimator.predict_proba(X)\n",
    "        probas /= len(self.estimators)\n",
    "        return probas\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict class for each object\n",
    "        :param X: Data\n",
    "        :return: array of class labels\n",
    "        \"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return np.argmax(probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe02b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
